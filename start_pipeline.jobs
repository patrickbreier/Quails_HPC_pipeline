#!/bin/bash


# This script starts the whole quail pipeline
# TODO
#  - make all things running for large data --> lazy loading (aicsimageio/tifffile)
#  - write file for time resolution at the end (thing for TM as well)
#  - write loops where I want to have options
#  - think about the restoring of final hierarchy if not in TA mode
#  - scripts to be written
#    - start_piv.sh
#    - piv.py
#    - how to figure out the number of time steps?
#    - define function for job starting to make this script shorter and more readable?


# look in dir, list the files and sorted by date and take the first one (use grep to look into this)
# to make it file name insensitive

prj="/projects/Quails/"


########################################################################################################################
#                                     Please make your changes here!                                                   #
########################################################################################################################

# all variables that have to be set before starting the pipeline
# template_update
number_of_tiles="12"
number_of_timesteps="28"  # actually its number of timesteps-1 because there is a step 0
script_dir="${prj}/scripts/pipeline/"

# image_conversion
csv_dir="${prj}/data/20210811_60x/adjust/"  # change!
csv_name="20210216_memgfpQuails_AJ_488_CSU_60x_current_" # filename without index and .vsi!

# N2V; if needed add learning rate and model-depth
N2V_model="${prj}/data/20211106_60x/model_first_long_training/" # "None" #either input path to trained N2V-model or input None in which case the data itself will be used to train a model point to folder that contains folder N2V
number_epochs="5"  # 100 is default; doesn't matter if model is set
steps_per_epoch="5"  # 400 is default; doesn't matter if model is set

# preprocessing
pp_in_dir="${csv_dir}/../clean/"
pp_out_dir="${csv_dir}/../preprocessed/"
pp_in_name="tile_"  # get this reported from finished job!
rb_radius="10"  # size of rolling ball kernel; should be set to at least the size of the largest object that is not part of the background
block_size="15"  # the size of the local region around a pixel for which the histogram is equalized. This size should be larger than the size of features to be preserved

# Projection
proj_type="MAX" # MAX or DPJ
path_to_proj_model="${prj}/DeepProjection_git/trained_networks/20211109_dorsal_closure.pth" # use this as long as there is no self_trained modelsqueue
proj_in_dir="${csv_dir}/../preprocessed/" # doesnt have to set since it is determined by earlier steps in the pipeline? --> hot to make earlier scripts set it?  trying to use 4dtsack version
proj_out_dir="${csv_dir}/../projected/" # doesnt have to set since it is determined by earlier steps in the pipeline?
proj_name="pp_tile_"

# Stitching
tiles_x="3"
tiles_y="4"
if [ ${proj_type} == "MAX" ]
then
  stitch_name_pattern="max_pp_tile_{ii}.tif" # doesnt have to set since it is determined by earlier steps in the pipeline?
elif [ ${proj_type} == "DPJ" ]; then
  stitch_name_pattern="proj_pp_tile_{ii}.tif" # doesnt have to set since it is determined by earlier steps in the pipeline?
else
  echo "invalid projection type"
  exit 1
fi
stitch_in_dir="${csv_dir}/../projected/" # doesnt have to set since it is determined by earlier steps in the pipeline?
stitch_out_dir="${csv_dir}/../stitched/" # doesnt have to set since it is determined by earlier steps in the pipeline?

# Segmentation
seg_in_dir="${csv_dir}/../stitched/timeseries/"
TA_mode="True" # False
cut_cells="None"

# restoring original folder structure
restore_dir="${csv_dir}/../stitched/timeseries/"

# copying Fused_XXX.tif to Fused_XXX/original.png
copy_dir="${csv_dir}/../stitched/timeseries/"  # this can be written as regex
copy_name="Fused_"

#cleanup
cleanup_dir="${script_dir}"


############################################# Starting Jobs ############################################################

# update all templates
template_update=$(sbatch ${prj}/scripts/pipeline/start_template_update.sh ${number_of_tiles} ${number_of_timesteps} ${script_dir})
template_update_job=$(echo "${template_update}" | awk '{ print $NF }')

sleep 5s

# first job has no dependencies
# start image conversion (array)
# potentially create a for loop that iterates over all tiles to process single/chucks of slices
mkdir ${csv_dir}/../raw # create dir to save the data in a dir of same hierarchy as csv_dir
image_conversion=$(sbatch --dependency=afterok:"${template_update_job}" ${prj}/scripts/pipeline/start_conversion_mod.sh ${csv_dir} ${csv_name})
image_conversion_job=$(echo "${image_conversion}" | awk '{ print $NF }')

# training for denoising starts only if needed, otherwise use existing model in specified directory
# training can only start once the array job for image-conversion has finished
# training is a single job
if [ ${N2V_model} == "None" ]
then
  mkdir ${csv_dir}/../model
  training_N2V=$(sbatch --dependency=afterok:"${image_conversion_job}" ${prj}/scripts/trainN2V.sh ${csv_dir} ${number_epochs} ${steps_per_epoch}) # tifs must be stored in a folder called raw inside this directory
  training_N2V_job=$(echo "${training_N2V}" | awk '{ print $NF }')
fi

# prediction can only start once the training has finished or, if specified, once the image-conversion is done
# prediction is another array
# will write output to a folder called clean
mkdir ${csv_dir}/../clean
if [ ${N2V_model} == "None" ]
then
  prediction_N2V=$(sbatch --dependency=afterok:"${training_N2V_job}" ${prj}/scripts/pipeline/predictN2V_mod.sh ${csv_dir} ${N2V_model})
  prediction_N2V_job=$(echo "${prediction_N2V}" | awk '{ print $NF }')
else
  prediction_N2V=$(sbatch --dependency=afterok:"${image_conversion_job}" ${prj}/scripts/pipeline/predictN2V_mod.sh ${csv_dir} ${N2V_model})
  prediction_N2V_job=$(echo "${prediction_N2V}" | awk '{ print $NF }')
fi

# after the prediction, some image manipulation will be done
# start as array
# what to parse here? a lot of parameters could be specified] by the earlier pipeline
mkdir ${csv_dir}/../preprocessed
preprocessing=$(sbatch --dependency=afterok:"${prediction_N2V_job}" ${prj}/scripts/pipeline/start_preprocessing_mod.sh ${pp_in_dir} ${pp_out_dir} ${pp_in_name} ${rb_radius} ${block_size})
preprocessing_job=$(echo "${preprocessing}" | awk '{ print $NF }')

# start the projection, can be parallelized for every single image if needed, otherwise for every tile?
# start once preprocessing is done
mkdir ${csv_dir}/../projected
if [ ${proj_type} == "DPJ" ]
then
  projection=$(sbatch --dependency=afterok:"${preprocessing_job}" ${prj}/scripts/pipeline/start_projection_mod.sh ${path_to_proj_model} ${proj_in_dir} ${proj_out_dir} ${proj_name})
  projection_job=$(echo "${projection}" | awk '{ print $NF }')
elif [ ${proj_type} == "MAX" ]; then
  projection=$(sbatch --dependency=afterok:"${preprocessing_job}" ${prj}/scripts/pipeline/start_max_mod.sh ${proj_in_dir} ${proj_out_dir} ${proj_name})
  projection_job=$(echo "${projection}" | awk '{ print $NF }')
elif [ ${proj_type} == "LOCALZ" ]; then
  projection=$(sbatch --dependency=afterok:"${preprocessing_job}" ${prj}/scripts/pipeline/start_localz_projection_mod.sh) # dependencies have to be figured out
  projection_job=$(echo "${projection}" | awk '{ print $NF }')
else
  echo "invalid projection type"
  exit 1
fi

# stitching; do this with fiji
# start after projections are done
mkdir ${csv_dir}/../stitched
mkdir ${csv_dir}/../stitched/timeseries
stitching=$(sbatch --dependency=afterok:"${projection_job}" ${prj}/scripts/pipeline/start_stitching.sh ${stitch_in_dir} ${stitch_out_dir} ${stitch_name_pattern} ${tiles_x} ${tiles_y})
stitching_job=$(echo "${stitching}" | awk '{ print $NF }')

# start the segmentation; should be parallelized for every single  image, otherwise this might take forever
# start once the stitching is done
# no subshell needed, since no other job is depending on it
segmentation=$(sbatch --dependency=afterok:"${stitching_job}" ${prj}/scripts/pipeline/start_segmentation_mod.sh ${seg_in_dir} ${TA_mode} ${cut_cells})  # ${number_of_timesteps} #segmentation input should be clear from earlier steps. still pass it?
segmentation_job=$(echo "${segmentation}" | awk '{ print $NF }')

# restore folder hierarchy
# this is tailored for TA Mode
restore=$(sbatch --dependency=afterok:"${segmentation_job}" ${prj}/scripts/pipeline/restore_hierarchy.sh ${restore_dir})
restore_job=$(echo "${restore}" | awk '{ print $NF }')

# copy Fused_XXX.tif as png in respective folder
copying=$(sbatch --dependency=afterok:"${restore_job}" ${prj}/scripts/pipeline/start_copy_png_mod.sh ${copy_dir} ${copy_name})
copy_job=$(echo "${copying}" | awk '{ print $NF }')

# start some PIV; can be done simultaneously to the segmentation
# this is just future stuff. no need to prioritize
# piv=$(sbatch --dependency=afterok:${stitching} start_piv.sh)

# remove the mod_scripts
sbatch --dependency=afterok:"${copy_job}" ${script_dir}/cleanup.sh ${cleanup_dir}

# show dependencies in squeue output after 5 seconds
sleep 5s
squeue -u "$USER"
